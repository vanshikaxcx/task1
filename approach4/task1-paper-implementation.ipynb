{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9685bb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-11T08:04:48.429120Z",
     "iopub.status.busy": "2026-01-11T08:04:48.428833Z",
     "iopub.status.idle": "2026-01-11T08:04:52.808207Z",
     "shell.execute_reply": "2026-01-11T08:04:52.807340Z"
    },
    "papermill": {
     "duration": 4.384996,
     "end_time": "2026-01-11T08:04:52.810159",
     "exception": false,
     "start_time": "2026-01-11T08:04:48.425163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\r\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (2.0.10)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools) (2.0.2)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.20.1)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.1rc0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.6.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm pycocotools torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8dffefb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:04:52.816551Z",
     "iopub.status.busy": "2026-01-11T08:04:52.816016Z",
     "iopub.status.idle": "2026-01-11T08:05:12.597787Z",
     "shell.execute_reply": "2026-01-11T08:05:12.596954Z"
    },
    "papermill": {
     "duration": 19.786923,
     "end_time": "2026-01-11T08:05:12.599634",
     "exception": false,
     "start_time": "2026-01-11T08:04:52.812711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8785d0b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:12.606107Z",
     "iopub.status.busy": "2026-01-11T08:05:12.605342Z",
     "iopub.status.idle": "2026-01-11T08:05:12.609618Z",
     "shell.execute_reply": "2026-01-11T08:05:12.609156Z"
    },
    "papermill": {
     "duration": 0.008655,
     "end_time": "2026-01-11T08:05:12.610876",
     "exception": false,
     "start_time": "2026-01-11T08:05:12.602221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.Resize((600, 600)))\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e984e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:12.616518Z",
     "iopub.status.busy": "2026-01-11T08:05:12.616287Z",
     "iopub.status.idle": "2026-01-11T08:05:12.864223Z",
     "shell.execute_reply": "2026-01-11T08:05:12.863493Z"
    },
    "papermill": {
     "duration": 0.252591,
     "end_time": "2026-01-11T08:05:12.865704",
     "exception": false,
     "start_time": "2026-01-11T08:05:12.613113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root, split, transforms=None):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.img_dir = os.path.join(root, 'images', split)\n",
    "        self.label_dir = os.path.join(root, 'labels', split)\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(self.img_dir) if f.endswith('.jpg')])\n",
    "        \n",
    "        self.voc_classes = [\n",
    "            \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \n",
    "            \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \n",
    "            \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "        ]\n",
    "        \n",
    "        self.target_classes = [\"person\", \"dog\", \"cat\", \"tvmonitor\", \"bird\"]\n",
    "        \n",
    "        self.class_to_idx = {cls: i + 1 for i, cls in enumerate(self.target_classes)}\n",
    "        \n",
    "        self.voc_to_model_map = {}\n",
    "        for idx, cls_name in enumerate(self.voc_classes):\n",
    "            if cls_name in self.target_classes:\n",
    "                self.voc_to_model_map[idx] = self.class_to_idx[cls_name]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt'))\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    data = line.strip().split()\n",
    "                    if len(data) < 5: continue\n",
    "                    \n",
    "                    cls_idx = int(data[0])\n",
    "                    \n",
    "                    if cls_idx in self.voc_to_model_map:\n",
    "                        cx, cy, bw, bh = map(float, data[1:5])\n",
    "                        \n",
    "                        xmin = (cx - bw/2) * w\n",
    "                        ymin = (cy - bh/2) * h\n",
    "                        xmax = (cx + bw/2) * w\n",
    "                        ymax = (cy + bh/2) * h\n",
    "                        \n",
    "                        boxes.append([xmin, ymin, xmax, ymax])\n",
    "                        labels.append(self.voc_to_model_map[cls_idx])\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            area = torch.zeros((0,), dtype=torch.float32)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79aa015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:12.871903Z",
     "iopub.status.busy": "2026-01-11T08:05:12.871389Z",
     "iopub.status.idle": "2026-01-11T08:05:12.881608Z",
     "shell.execute_reply": "2026-01-11T08:05:12.880994Z"
    },
    "papermill": {
     "duration": 0.014778,
     "end_time": "2026-01-11T08:05:12.882911",
     "exception": false,
     "start_time": "2026-01-11T08:05:12.868133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class CCTTBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.c_stage1 = self._make_c_layer(96, 96, 2)\n",
    "        self.c_stage2 = self._make_c_layer(96, 192, 2, stride=2)\n",
    "        \n",
    "        swin_config = timm.models.swin_transformer.SwinTransformer(\n",
    "            img_size=224, patch_size=4, in_chans=3, num_classes=0,\n",
    "            embed_dim=96, depths=[2, 2, 9, 3], num_heads=[3, 6, 12, 24],\n",
    "            window_size=7\n",
    "        )\n",
    "        \n",
    "        self.t_stage3 = swin_config.layers[2]\n",
    "        self.t_stage4 = swin_config.layers[3]\n",
    "        \n",
    "        self.out_channels = 768\n",
    "\n",
    "    def _make_c_layer(self, in_c, out_c, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ConvBlock(in_c, out_c, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ConvBlock(out_c, out_c))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)        \n",
    "        c1 = self.c_stage1(x)   \n",
    "        c2 = self.c_stage2(c1)  \n",
    "        \n",
    "        x_in = c2.permute(0, 2, 3, 1) \n",
    "        \n",
    "        t3 = self.t_stage3(x_in) \n",
    "        t4 = self.t_stage4(t3)   \n",
    "        \n",
    "        t4_out = t4.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return {\n",
    "            \"0\": t4_out \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5647b3e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:12.888687Z",
     "iopub.status.busy": "2026-01-11T08:05:12.888238Z",
     "iopub.status.idle": "2026-01-11T08:05:12.893192Z",
     "shell.execute_reply": "2026-01-11T08:05:12.892532Z"
    },
    "papermill": {
     "duration": 0.009378,
     "end_time": "2026-01-11T08:05:12.894493",
     "exception": false,
     "start_time": "2026-01-11T08:05:12.885115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cctt_model(num_classes):\n",
    "    backbone = CCTTBackbone()\n",
    "    backbone.out_channels = 768\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    \n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    \n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool_init_fn=None,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b150d6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:12.899865Z",
     "iopub.status.busy": "2026-01-11T08:05:12.899503Z",
     "iopub.status.idle": "2026-01-11T08:05:12.906060Z",
     "shell.execute_reply": "2026-01-11T08:05:12.905387Z"
    },
    "papermill": {
     "duration": 0.010713,
     "end_time": "2026-01-11T08:05:12.907365",
     "exception": false,
     "start_time": "2026-01-11T08:05:12.896652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_calibration(model, dataloader, device, alpha=0.25):\n",
    "    model.train()\n",
    "    images, targets = next(iter(dataloader))\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss_dict = model(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    losses.backward()\n",
    "\n",
    "    layer_norms = []\n",
    "    layers = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and param.requires_grad:\n",
    "            norm = torch.norm(param.grad)\n",
    "            layer_norms.append(norm.item())\n",
    "            layers.append(param)\n",
    "    \n",
    "    if len(layer_norms) == 0:\n",
    "        return\n",
    "\n",
    "    c_tilde = np.exp(np.mean(np.log(np.array(layer_norms) + 1e-8)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param, norm in zip(layers, layer_norms):\n",
    "            if norm > 0:\n",
    "                r_k = (norm / c_tilde) ** alpha\n",
    "                param.data.mul_(r_k)\n",
    "    \n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bd93cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:12.913055Z",
     "iopub.status.busy": "2026-01-11T08:05:12.912441Z",
     "iopub.status.idle": "2026-01-11T08:05:12.918959Z",
     "shell.execute_reply": "2026-01-11T08:05:12.918299Z"
    },
    "papermill": {
     "duration": 0.010767,
     "end_time": "2026-01-11T08:05:12.920241",
     "exception": false,
     "start_time": "2026-01-11T08:05:12.909474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n",
    "    for images, targets in pbar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_total += losses.item()\n",
    "        pbar.set_postfix({'Loss': losses.item()})\n",
    "        \n",
    "    return loss_total / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        outputs = model(images)\n",
    "        metric.update(outputs, targets)\n",
    "        \n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50249e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:12.926042Z",
     "iopub.status.busy": "2026-01-11T08:05:12.925341Z",
     "iopub.status.idle": "2026-01-11T08:05:13.074667Z",
     "shell.execute_reply": "2026-01-11T08:05:13.073970Z"
    },
    "papermill": {
     "duration": 0.153535,
     "end_time": "2026-01-11T08:05:13.076052",
     "exception": false,
     "start_time": "2026-01-11T08:05:12.922517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5717\n",
      "Validation samples: 5823\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/kaggle/input/pascal-voc-2012/VOC2012\"\n",
    "\n",
    "if not os.path.exists(root_dir):\n",
    "    raise FileNotFoundError(f\"Could not find dataset at {root_dir}\")\n",
    "\n",
    "dataset_train = VOCDataset(root_dir, \"train\", get_transforms(True))\n",
    "dataset_val = VOCDataset(root_dir, \"val\", get_transforms(False))\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=2, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    dataset_val, \n",
    "    batch_size=2, \n",
    "    shuffle=False, \n",
    "    num_workers=2, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(dataset_train)}\")\n",
    "print(f\"Validation samples: {len(dataset_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c22e239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T08:05:13.082046Z",
     "iopub.status.busy": "2026-01-11T08:05:13.081760Z",
     "iopub.status.idle": "2026-01-11T10:48:58.868145Z",
     "shell.execute_reply": "2026-01-11T10:48:58.867455Z"
    },
    "papermill": {
     "duration": 9825.791517,
     "end_time": "2026-01-11T10:48:58.869974",
     "exception": false,
     "start_time": "2026-01-11T08:05:13.078457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/2344681862.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 0:   0%|          | 0/2859 [00:00<?, ?it/s]/tmp/ipykernel_25/2344681862.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 0: 100%|██████████| 2859/2859 [11:32<00:00,  4.13it/s, Loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.2890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2859/2859 [11:35<00:00,  4.11it/s, Loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2859/2859 [11:35<00:00,  4.11it/s, Loss=0.0352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.2384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2859/2859 [11:35<00:00,  4.11it/s, Loss=0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.2406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2912/2912 [07:38<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2859/2859 [11:36<00:00,  4.11it/s, Loss=0.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2859/2859 [11:36<00:00,  4.11it/s, Loss=0.0882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2859/2859 [11:36<00:00,  4.11it/s, Loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.2347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2859/2859 [11:37<00:00,  4.10it/s, Loss=0.0651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.2336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2912/2912 [08:01<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2859/2859 [11:38<00:00,  4.09it/s, Loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.2320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2859/2859 [11:38<00:00,  4.09it/s, Loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.2269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2859/2859 [11:38<00:00,  4.09it/s, Loss=0.0578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.2244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2859/2859 [11:39<00:00,  4.09it/s, Loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 0.2225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2912/2912 [07:52<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.0183\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = get_cctt_model(num_classes=6)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(params, lr=0.0001, weight_decay=0.05)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "gradient_calibration(model, data_loader_train, device)\n",
    "\n",
    "num_epochs = 12\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    pbar = tqdm(data_loader_train, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_total += losses.item()\n",
    "        pbar.set_postfix({'Loss': losses.item()})\n",
    "        \n",
    "    avg_loss = loss_total / len(data_loader_train)\n",
    "    print(f\"Epoch {epoch} Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    checkpoint_path = f\"cctt_voc_checkpoint_epoch_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    if (epoch + 1) % 4 == 0:\n",
    "        mAP = evaluate(model, data_loader_val, device)\n",
    "        print(f\"mAP: {mAP['map']:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"cctt_voc_final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d0460",
   "metadata": {
    "papermill": {
     "duration": 3.238363,
     "end_time": "2026-01-11T10:49:05.166344",
     "exception": false,
     "start_time": "2026-01-11T10:49:01.927981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7807229,
     "sourceId": 12381551,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9865.380158,
   "end_time": "2026-01-11T10:49:11.196497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-11T08:04:45.816339",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
